{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224U Final Project\n",
    "\n",
    "First necessary imports and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from torch_rnn_classifier import a\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import re\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HateSpeech_HOME = os.path.join('CS224U-Final-Project', 'Data')\n",
    "GLOVE_HOME = os.path.join('data', 'glove.6B')\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_reader():\n",
    "    \"\"\"\n",
    "    Iterator for reading in dataset adapted by code provided by the generous CS224U teaching staff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the file to be read.\n",
    "\n",
    "\n",
    "    dedup : bool\n",
    "        If True, only one copy of each (text, tabel) pair is included.\n",
    "        This mainly affects the train set, though there is one repeated\n",
    "        example in the dev set.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    pd.DataFrame with columns ['example_id', 'text', 'hate_speech', label\n",
    "\n",
    "    \"\"\"\n",
    "    src_filename = os.path.join(HateSpeech_HOME, 'MasterDataSet.csv')\n",
    "    df = pd.read_csv(src_filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateSpeechDev = dataset_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'example_id': 5222,\n",
       "  'sentence': '@Yg_Trece I agree. You never know she may have been begging to get dropped, makes her pussy wet guaranteed. She married him right after !',\n",
       "  'label': 'neutral'},\n",
       " {'example_id': 7255,\n",
       "  'sentence': '@vewxyz this bitch........',\n",
       "  'label': 'neutral'},\n",
       " {'example_id': 14810,\n",
       "  'sentence': 'RT @CuhCuhCuh: I got NO love for bitches or bitch niggas cuh',\n",
       "  'label': 'hate'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateSpeechDev.sample(3, random_state=1).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    22476\n",
       "hate       11680\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateSpeechDev.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(hateSpeechDev, test_size=0.7)\n",
    "test_small, test_large = train_test_split(test, test_size = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    6789\n",
       "hate       3457\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    15687\n",
       "hate        8223\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Baseline: Linear Softmax Classifier with Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(text):\n",
    "    \"\"\"\n",
    "    The basis for a unigrams feature function. Downcases all tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The example to represent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        A map from strings to their counts in `tree`. (Counter maps a\n",
    "        list to a dict of counts of the elements in that list.)\n",
    "\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    return Counter(text.lower().split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_phi(text):\n",
    "    \"\"\"\n",
    "    The basis for a bigrams feature function. Downcases all tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The example to represent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        A map from tuples to their counts in `text`.\n",
    "\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    toks = text.lower().split()\n",
    "    left = [utils.START_SYMBOL] + toks\n",
    "    right = toks + [utils.END_SYMBOL]\n",
    "    grams = list(zip(left, right))\n",
    "    return Counter(grams) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining both Uni and bi grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_and_bigrams_phi(text):\n",
    "    \n",
    "    return unigrams_phi(text) + bigrams_phi(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_with_hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    A MaxEnt model of dataset with hyperparameter cross-validation.\n",
    "\n",
    "    Some notes:\n",
    "\n",
    "    * 'fit_intercept': whether to include the class bias feature.\n",
    "    * 'C': weight for the regularization term (smaller is more regularized).\n",
    "    * 'penalty': type of regularization -- roughly, 'l1' ecourages small\n",
    "      sparse models, and 'l2' encourages the weights to conform to a\n",
    "      gaussian prior distribution.\n",
    "    * 'class_weight': 'balanced' adjusts the weights to simulate a\n",
    "      balanced class distribution, whereas None makes no adjustment.\n",
    "\n",
    "    Other arguments can be cross-validated; see\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "\n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.linear_model.LogisticRegression\n",
    "        A trained model instance, the best model found.\n",
    "\n",
    "    \"\"\"\n",
    "    basemod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='auto')\n",
    "    cv = 5\n",
    "    param_grid = {\n",
    "        'C': [0.8, 1.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'class_weight': ['balanced', None]}\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, basemod, cv, param_grid)\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_experiment_uni = sst.experiment(\n",
    "#     train,   \n",
    "#     unigrams_phi,                 \n",
    "#     fit_softmax_with_hyperparameter_search,      \n",
    "#     assess_dataframes=[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_experiment_bi = sst.experiment(\n",
    "#     train,   \n",
    "#     bigrams_phi,                 \n",
    "#     fit_softmax_with_hyperparameter_search,      \n",
    "#     assess_dataframes=[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Best score: 0.751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.713     0.604     0.654     10463\n",
      "     neutral      0.811     0.875     0.841     20278\n",
      "\n",
      "    accuracy                          0.782     30741\n",
      "   macro avg      0.762     0.739     0.748     30741\n",
      "weighted avg      0.777     0.782     0.778     30741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment_uni_bi = sst.experiment(\n",
    "    train,   \n",
    "    bigrams_phi,                 \n",
    "    fit_softmax_with_hyperparameter_search,      \n",
    "    assess_dataframes=[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(text):\n",
    "    text = preprocess(text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_phi(text, lookup, np_func=np.mean):\n",
    "    \"\"\"Represent `tree` as a combination of the vector of its words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "\n",
    "    lookup : dict\n",
    "        From words to vectors.\n",
    "\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise,\n",
    "        like `np.mean`, `np.sum`, or `np.prod`. The requirement is that\n",
    "        the function take `axis=0` as one of its arguments (to ensure\n",
    "        columnwise combination) and that it return a vector of a\n",
    "        fixed length, no matter what the size of the tree is.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `X.shape[1]`\n",
    "\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    allvecs = np.array([lookup[w] for w in text.split() if w in lookup])\n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:\n",
    "        feats = np_func(allvecs, axis=0)\n",
    "    return feats\n",
    "\n",
    "def glove_phi(text, np_func=np.mean):\n",
    "    return vsm_phi(text, glove_lookup, np_func=np_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_with_hyperparameter_search(X, y):\n",
    "    sst_train_vocab = utils.get_vocab(X, mincount=2)\n",
    "    glove_embedding, sst_glove_vocab = utils.create_pretrained_embedding(glove_lookup, sst_train_vocab)\n",
    "    basemod = TorchRNNClassifier(\n",
    "        sst_glove_vocab,\n",
    "        embedding=glove_embedding,\n",
    "        batch_size=25,  \n",
    "        bidirectional=True,\n",
    "        early_stopping=True)\n",
    "\n",
    "    # There are lots of other parameters and values we could\n",
    "    # explore, but this is at least a solid start:\n",
    "    param_grid = {\n",
    "        'embed_dim': [75, 100],\n",
    "        'hidden_dim': [75, 100],\n",
    "        'eta': [0.001, 0.01]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, basemod, cv=3, param_grid=param_grid)\n",
    "\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.53592498919124415"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'embed_dim': 100, 'eta': 0.01, 'hidden_dim': 75}\n",
      "Best score: 0.766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.721     0.674     0.697     10463\n",
      "     neutral      0.837     0.865     0.851     20278\n",
      "\n",
      "    accuracy                          0.800     30741\n",
      "   macro avg      0.779     0.770     0.774     30741\n",
      "weighted avg      0.798     0.800     0.799     30741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_experiment = sst.experiment(\n",
    "    train,\n",
    "    rnn_phi,\n",
    "    fit_rnn_with_hyperparameter_search,\n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_dataframes=[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_experiment_hyperparams = rnn_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_experiment = sst.experiment(\n",
    "#     train,\n",
    "#     vsm_phi,\n",
    "#     fit_rnn_with_hyperparameter_search,\n",
    "#     vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "#     assess_dataframes=[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_shallow_neural_classifier_with_hyperparameter_search(X, y):\n",
    "    basemod = TorchShallowNeuralClassifier(\n",
    "        early_stopping=True \n",
    "        ) \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim': [50,100,200],\n",
    "        'hidden_activation': [nn.ReLU(), nn.Tanh()],\n",
    "        }\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(X, y, basemod, cv, param_grid)\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 21. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.127093225717544567"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'hidden_activation': ReLU(), 'hidden_dim': 100}\n",
      "Best score: 0.710\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.722     0.479     0.576     10463\n",
      "     neutral      0.771     0.905     0.833     20278\n",
      "\n",
      "    accuracy                          0.760     30741\n",
      "   macro avg      0.746     0.692     0.704     30741\n",
      "weighted avg      0.754     0.760     0.745     30741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_shallow_neural_experiment = sst.experiment(\n",
    "    train,   \n",
    "    bigrams_phi,                 \n",
    "    fit_shallow_neural_classifier_with_hyperparameter_search,      \n",
    "    assess_dataframes=[test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.8, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Best score: 0.731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.584     0.710     0.641     10463\n",
      "     neutral      0.832     0.739     0.783     20278\n",
      "\n",
      "    accuracy                          0.729     30741\n",
      "   macro avg      0.708     0.725     0.712     30741\n",
      "weighted avg      0.747     0.729     0.735     30741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_linearRegression_glove_phi = sst.experiment(train,\n",
    "                                                glove_phi,\n",
    "                                                fit_softmax_with_hyperparameter_search,\n",
    "                                                assess_dataframes=test,\n",
    "                                                vectorize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni and Bigram NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "def fit_nb_classifier_with_hyperparameter_search(X, y):\n",
    "    rescaler = TfidfTransformer()\n",
    "    mod = MultinomialNB()\n",
    "\n",
    "    pipeline = Pipeline([('scaler', rescaler), ('model', mod)])\n",
    "\n",
    "    # Access the alpha and fit_prior parameters of `mod` with\n",
    "    # `model__alpha` and `model__fit_prior`, where \"model\" is the\n",
    "    # name from the Pipeline. Use 'passthrough' to optionally\n",
    "    # skip TF-IDF.\n",
    "    param_grid = {\n",
    "        'model__fit_prior': [True, False],\n",
    "        'scaler': ['passthrough', rescaler],\n",
    "        'model__alpha': [0.1, 0.2, 0.4, 0.8, 1.0, 1.2]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5)\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'model__alpha': 1.2, 'model__fit_prior': True, 'scaler': 'passthrough'}\n",
      "Best score: 0.739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.684     0.571     0.623     10463\n",
      "     neutral      0.796     0.864     0.829     20278\n",
      "\n",
      "    accuracy                          0.764     30741\n",
      "   macro avg      0.740     0.718     0.726     30741\n",
      "weighted avg      0.758     0.764     0.759     30741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_nb_experiment_xval = sst.experiment(\n",
    "    train,\n",
    "    bigrams_phi,\n",
    "    fit_nb_classifier_with_hyperparameter_search,\n",
    "    assess_dataframes=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def fit_svm_classifier_with_hyperparameter_search(X, y):\n",
    "    rescaler = TfidfTransformer()\n",
    "    mod = LinearSVC(loss='squared_hinge', penalty='l2')\n",
    "\n",
    "    pipeline = Pipeline([('scaler', rescaler), ('model', mod)])\n",
    "\n",
    "    # Access the alpha parameter of `mod` with `mod__alpha`,\n",
    "    # where \"model\" is the name from the Pipeline. Use\n",
    "    # 'passthrough' to optionally skip TF-IDF.\n",
    "    param_grid = {\n",
    "        'scaler': ['passthrough', rescaler],\n",
    "        'model__C': [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5)\n",
    "    \n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'model__C': 0.1, 'scaler': 'passthrough'}\n",
      "Best score: 0.753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.732     0.568     0.640      1146\n",
      "     neutral      0.790     0.887     0.835      2098\n",
      "\n",
      "    accuracy                          0.774      3244\n",
      "   macro avg      0.761     0.727     0.738      3244\n",
      "weighted avg      0.769     0.774     0.766      3244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_experiment_xval = sst.experiment(\n",
    "    train,\n",
    "    bigrams_phi,\n",
    "    fit_svm_classifier_with_hyperparameter_search,\n",
    "    assess_dataframes=test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Getting Optimal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_experiment_optimal = softmax_experiment_uni_bi['model']\n",
    "rnn_experiment_optimal = rnn_experiment_hyperparams['model']\n",
    "shallow_experiment_optimal = torch_shallow_neural_experiment['model']\n",
    "nb_experiment_optimal = unigram_nb_experiment_xval['model']\n",
    "svm_experiment_optimal = svm_experiment_xval['model']\n",
    "\n",
    "softmax_experiment_optimal_data = softmax_experiment_uni_bi['assess_datasets']\n",
    "rnn_experiment_optimal_data = rnn_experiment_hyperparams['assess_datasets']\n",
    "shallow_experiment_optimal_data = torch_shallow_neural_experiment['assess_datasets']\n",
    "nb_experiment_optimal_data = unigram_nb_experiment_xval['assess_datasets']\n",
    "svm_experiment_optimal_data = svm_experiment_xval['assess_datasets']\n",
    "\n",
    "del softmax_experiment_uni_bi\n",
    "del rnn_experiment_hyperparams\n",
    "del torch_shallow_neural_experiment\n",
    "del unigram_nb_experiment_xval\n",
    "del svm_experiment_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.754     0.625     0.683       861\n",
      "     neutral      0.807     0.885     0.844      1530\n",
      "\n",
      "    accuracy                          0.791      2391\n",
      "   macro avg      0.780     0.755     0.764      2391\n",
      "weighted avg      0.788     0.791     0.786      2391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_optimized_softmax(X, y):\n",
    "    basemod = LogisticRegression(\n",
    "        C = 1,\n",
    "        class_weight = 'balanced',\n",
    "        penalty = \"l2\",\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='auto')\n",
    "    basemod.fit(X, y)\n",
    "    return basemod\n",
    "\n",
    "softmax_rerun = sst.experiment(\n",
    "    train,\n",
    "    bigrams_phi,\n",
    "    fit_optimized_softmax,\n",
    "    assess_dataframes=test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 11.27336708268922"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.867     0.619     0.722       861\n",
      "     neutral      0.815     0.946     0.876      1530\n",
      "\n",
      "    accuracy                          0.829      2391\n",
      "   macro avg      0.841     0.783     0.799      2391\n",
      "weighted avg      0.834     0.829     0.821      2391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_rnn_optimized(X, y):\n",
    "    sst_train_vocab = utils.get_vocab(X, mincount=2)\n",
    "    glove_embedding, sst_glove_vocab = utils.create_pretrained_embedding(glove_lookup, sst_train_vocab)\n",
    "    basemod = TorchRNNClassifier(\n",
    "        sst_glove_vocab,\n",
    "        embedding=glove_embedding,\n",
    "        embed_dim = 100,\n",
    "        eta = .01,\n",
    "        hidden_dim = 75,\n",
    "        batch_size=25,  \n",
    "        bidirectional=True,\n",
    "        early_stopping=True)\n",
    "    basemod.fit(X, y)\n",
    "    return basemod\n",
    "    \n",
    "rnn_rerun = sst.experiment(\n",
    "    train,\n",
    "    rnn_phi,\n",
    "    fit_rnn_optimized,\n",
    "    vectorize = False,\n",
    "    assess_dataframes = test_small\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = train_test_split(hateSpeechDev, test_size=0.85)\n",
    "test_y, test_large_y = train_test_split(test_x, test_size = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    3347\n",
       "hate       1776\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    1923\n",
       "hate        980\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 22. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.08859242871403694"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.765     0.643     0.698       831\n",
      "     neutral      0.825     0.895     0.858      1560\n",
      "\n",
      "    accuracy                          0.807      2391\n",
      "   macro avg      0.795     0.769     0.778      2391\n",
      "weighted avg      0.804     0.807     0.803      2391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_shallow_optimized(X, y):\n",
    "    basemod = TorchShallowNeuralClassifier(\n",
    "        early_stopping=True,\n",
    "        hidden_activation = nn.ReLU(),\n",
    "        hidden_dim = 100\n",
    "        ) \n",
    "    basemod.fit(X, y)\n",
    "    return basemod\n",
    "\n",
    "shallow_rerun = sst.experiment(\n",
    "    train_x,   \n",
    "    bigrams_phi,                 \n",
    "    fit_shallow_optimized,      \n",
    "    assess_dataframes=[test_y]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.619     0.676     0.646       861\n",
      "     neutral      0.808     0.766     0.786      1530\n",
      "\n",
      "    accuracy                          0.734      2391\n",
      "   macro avg      0.713     0.721     0.716      2391\n",
      "weighted avg      0.740     0.734     0.736      2391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_softmax_glove_optimized(X, y):\n",
    "    basemod = LogisticRegression(\n",
    "        C = .8,\n",
    "        class_weight = 'balanced',\n",
    "        penalty = \"l2\",\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='auto')\n",
    "    basemod.fit(X, y)\n",
    "    return basemod\n",
    "    \n",
    "\n",
    "softmax_glove_phi_rerun = sst.experiment(train,\n",
    "                                        glove_phi,\n",
    "                                        fit_softmax_glove_optimized,\n",
    "                                        assess_dataframes=test_small,\n",
    "                                        vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'model__alpha': 1.2, 'model__fit_prior': True, 'scaler': 'passthrough'}\n",
      "Best score: 0.727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.785     0.615     0.690       831\n",
      "     neutral      0.816     0.910     0.861      1560\n",
      "\n",
      "    accuracy                          0.808      2391\n",
      "   macro avg      0.801     0.763     0.775      2391\n",
      "weighted avg      0.805     0.808     0.801      2391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "def fit_nb_optimized(X, y):\n",
    "    rescaler = TfidfTransformer()\n",
    "    mod = MultinomialNB()\n",
    "    pipeline = Pipeline(\n",
    "        [('scaler', rescaler), ('model', mod)]\n",
    "    )\n",
    "    param_grid = {\n",
    "        'model__fit_prior': [True],\n",
    "        'scaler': ['passthrough'],\n",
    "        'model__alpha': [1.2]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5)\n",
    "    return bestmod\n",
    "\n",
    "\n",
    "nb_rerun = sst.experiment(\n",
    "    train_x,\n",
    "    bigrams_phi,\n",
    "    fit_nb_optimized,\n",
    "    assess_dataframes= test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'model__C': 0.1, 'scaler': 'passthrough'}\n",
      "Best score: 0.730\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate      0.725     0.558     0.631       980\n",
      "     neutral      0.798     0.892     0.843      1923\n",
      "\n",
      "    accuracy                          0.779      2903\n",
      "   macro avg      0.761     0.725     0.737      2903\n",
      "weighted avg      0.773     0.779     0.771      2903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def fit_svm_classifier_with_hyperparameter_search(X, y):\n",
    "    rescaler = TfidfTransformer()\n",
    "    mod = LinearSVC(loss='squared_hinge', penalty='l2')\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [('scaler', rescaler), ('model', mod)]\n",
    "    )\n",
    "    \n",
    "    param_grid = {\n",
    "        'scaler': ['passthrough'],\n",
    "        'model__C': [.10]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5)\n",
    "    return bestmod\n",
    "\n",
    "svm_experiment_xval = sst.experiment(\n",
    "    train_x,\n",
    "    bigrams_phi,\n",
    "    fit_svm_classifier_with_hyperparameter_search,\n",
    "    assess_dataframes=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Assess Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for i, dataset in enumerate(experiment['assess_datasets']):\n",
    "        df = pd.DataFrame({\n",
    "            'raw_examples': dataset['raw_examples'],\n",
    "            'predicted': experiment['predictions'][i],\n",
    "            'gold': dataset['y']})\n",
    "        df['correct'] = df['predicted'] == df['gold']\n",
    "        df['dataset'] = i\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_analysis = find_errors(softmax_rerun)\n",
    "rnn_analysis = find_errors(rnn_rerun)\n",
    "shallow_analysis = find_errors(shallow_rerun)\n",
    "nb_analysis =  find_errors(nb_rerun)\n",
    "svm_analysis = find_errors(svm_experiment_xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "@The_Gambit Ha. He ain't welcome in Washington State or Nashville. Serpas is straight trash.\n",
      "======================================================================\n",
      "1. I thought GOG was turning far left?  Is there an internal fight going on there?  \n",
      "======================================================================\n",
      "RT @SammyLightning_: @TNasty4Teen we find turtle; we find the hoes!\n",
      "======================================================================\n",
      "RT @SoloDahSystem: As in rubbing her booty on another mans penis. Sure RT @PacDaGoat: Fellas you cool with your girl dancing with other nig&#8230;\n",
      "======================================================================\n",
      "@ImToBlame listen cuntrag whore bitch, shit be expensive as fuck, man. We had to do PR first that one year, this year was the wedding so.\n",
      "======================================================================\n",
      "#milesthompson #goauche #wetback #ca http://t.co/g3Ze8bqLPG\n",
      "======================================================================\n",
      "RT @NoWomanIsRight: You can be a good girl all you want and those hoes still gonna get us niggas attention from time to time\n",
      "======================================================================\n",
      "@TheEllenShow stfu trash\n",
      "======================================================================\n",
      "RT @TrueNovacane: Only pussy niggas wouldn't fuck with a chick if she has stretch marks..\n",
      "======================================================================\n",
      "My cats a bitch niggah.\n",
      "======================================================================\n",
      "RT @TAXSTONE: Niggaz like all them naked bitches on IG pics NIGGAZ DONT LOVE EM\n",
      "======================================================================\n",
      "I'm not fuckin wit October She on some new shit , bitch cold\n",
      "======================================================================\n",
      "Even my moms knows I don't fuck with ghetto people\n",
      "======================================================================\n",
      "You can't play me for a fool my niggah\n",
      "======================================================================\n",
      "@Cody_smith64 true! I miss yall man, yall are my brothers. Is mikey starting? And what about the chink? Lmfao\n",
      "======================================================================\n",
      "Cut deses bitches off like Hibachi &#9986;&#65039;\n",
      "======================================================================\n",
      "Bitches be like im nt beefing over a niggah smh\n",
      "======================================================================\n",
      "\"You don't like us colored folks, what the he'll you're not colored you white fucks\" &#128514;&#128514;&#128514;&#128514;\n",
      "======================================================================\n",
      "&#8220;@flowerlela: Fucking slut&#8221;\n",
      "\n",
      "FOREAL, fuck dat bitch.\n",
      "======================================================================\n",
      "I really get upset when my lightskin friends have another cute dark skin sidekick..like Im supposed to be the only cute darkie u hang wit &#128545;\n"
     ]
    }
   ],
   "source": [
    "analysis = rnn_analysis.merge(\n",
    "    shallow_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})\n",
    "\n",
    "# Examples where the rnn model is correct, the SHALLOW is not,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "for ex in error_group['raw_examples'].sample(20, random_state=1):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "@imgooby you're not funny trash\n",
      "======================================================================\n",
      "Spending money on these hoes , nigga you aint fuck SHIT &#10071;&#65039;\n",
      "======================================================================\n",
      "They got some trashy ass hoes out here\n",
      "======================================================================\n",
      "RT @MyFriendsRap: You hoes need to stop dressing y'all kids as Migos and Rappers that y'all fucked. Lil nigga just wanted to be a power ran&#8230;\n",
      "======================================================================\n",
      "@DeezNutsDoe you dirty little bitch\n",
      "======================================================================\n",
      "1. If the voice of your “resistance” is a mentally-ill drag queen, then it’s safe to say your resistance is likely equally deranged.\n",
      "======================================================================\n",
      "Your bitch want to fuck the squad &#128527;&#128076;\n",
      "======================================================================\n",
      "1. The Google Walkout is yet another example of how no one is safe from bullying and intimidation by social justice advocates, not even companies that fell over backwards to try and cater to their wishes, wants and needs.  Well, Google, while you're getting your education on these twats, you could always say hi to Wil Wheaton or the other social justice wannabe leaders that saw the worm turn on them. I'm sure they'll have lots to say about how to handle the capricious mood of the mob.\n",
      "======================================================================\n",
      "I had wack dick until I started messing with white hoes. They stuck around &amp; let a young goon practice on em.\n",
      "======================================================================\n",
      "1\n",
      "======================================================================\n",
      "when Em said \"I'm on my straight grizzly, so why would I buy you a gay ass teddy bear bitch youre already bi-polar.\" http://t.co/FQquXTxHBo\n",
      "======================================================================\n",
      "Peyton Manning is a goon fuck Eli's pussy ass\n",
      "======================================================================\n",
      "That bitch was worried about fines? Fuck you. My best friend got jumped by 6 cock suckers. Fines should be the least of your worries,\n",
      "======================================================================\n",
      "1. He is RIGHT, but you pathetic Trump apologists will keep sucking on his cock for shekels. Trump's NAfta \"change\" isn't any change. IT is the SAME shit and some parts are EVEN WORSE and much WORSE!!!!!! Keep sucking on his cock though, TRS faggots! SAD!   https://www.youtube.com/watch?v=YG0_mKJR6CQ\n",
      "======================================================================\n",
      "@SteveWorks4You bush took 3 times as many vacations.. republican teabaggers are pusses....wah\n"
     ]
    }
   ],
   "source": [
    "# Examples where the rnn model is wrong, the SHALLOW is right,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "for ex in error_group['raw_examples'].sample(15, random_state=1):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. How much of a kick in the cunt is it when a 14 yr old boy breaks up with you?\n",
      "======================================================================\n",
      "1. I see my insta/fb friends from high school (I went to a fairly preppy upper middle class school) and 90% of the girls post shit like “Believe Women.  No questions asked.” and all I can think is you poor daft cunt\n",
      "======================================================================\n",
      "@mlvassallo Oh really? fags are gross You aids infected sick fuckers\n",
      "======================================================================\n",
      "Chris Bosh bout to disgrace the trophy like a real faggot smh\n",
      "======================================================================\n",
      "What's so good about Prom? All I see is another faggot ass High School dance.\n",
      "======================================================================\n",
      "1. Ok where are all my #MAGA #RedPilled#Trump2020 #RedWave people at?? FOLLOW ME!!! Got tired of being zucced so I'm here.....   I'm sorry but i just have to do this out of Facebook suppression....LIBERALS ARE TRANNY FAGGOT BITCH ASS NIGGA DYKE PIG FUCKING MUSLIM HOOKNOSE WETBACKS!!!    AHHHH felt so good. #FreeSpeech\n",
      "======================================================================\n",
      "1. Southwest Airlines passenger removed for calling flight attendant a nigger. I think I just found my new BFF!  https://www.foxnews.com/travel/southwest-airlines-passenger-removed-for-calling-flight-attendant-the-n-word-delaying-plane\n",
      "======================================================================\n",
      "1. Ya but retard leftist just call everyone a racist.  So it's fact against the lefts labels. \n",
      "======================================================================\n",
      "These niggas groupies they act worse than these hoes do\n",
      "======================================================================\n",
      "1. I just walked out to my car and two filthy nigger beasts are sitting in the parking lot eating chicken and throwing their BBQ sauce containers out the window and splashing shit all over the car next to it. MINE. I so wish I wasn’t a girl right now.\n"
     ]
    }
   ],
   "source": [
    "# Examples where the rnn model is correct, the SHALLOW is not,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "for ex in error_group['raw_examples'].sample(10, random_state=69):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "they probably hate me even more because not only do I look white (when I'm technically a poc), but I have white hair now I'm double cracker\n",
      "======================================================================\n",
      "What's so good about Prom? All I see is another faggot ass High School dance.\n",
      "======================================================================\n",
      "@SchulzGrayson @Dswizzle3 and i dont have a mom bitch ass nigga u better shut your fucking mouth before i do it for you with duck ass\n",
      "======================================================================\n",
      "@BarneyFranken @jennykutner @Salon salon is is chuck full of butthurt liberal retards\n",
      "======================================================================\n",
      "When that dyke almost came for me when i didn't send for her &lt;&lt;&lt;&lt;\n",
      "======================================================================\n",
      "&#8220;@iamkrause: No need to thank me, killing niggers is it's own reward.&#8221; \n",
      "\n",
      "Dis my pal from our graduation when we was 26\n",
      "======================================================================\n",
      "RT @UNCLERUCKUS_BD: Being a nigger isn't a color it's a state of mind\n",
      "======================================================================\n",
      "Fucking gook\n",
      "======================================================================\n",
      "@HeauxmerSimpson I'm jus tryna vaca away from the niggers bro.\n",
      "======================================================================\n",
      "1. trolling 'pro women' stories on my local news website, DUDE there are so many.....'women are victims of abuse'.......'women want credit for doing jobs' these articles are endless and fucking retarded I understand why men hate women.I hate women and I am one.I only got love for pro white ladies cuz they are fearless and dont play the victim card\n",
      "======================================================================\n",
      "I got bitches on my dick\n",
      "\n",
      "They just ain't here yet nigga\n",
      "======================================================================\n",
      "@red_daddy17 @lilJohn_Fnv9 and dis wus after i offered to help faggot ass nigga get custody of his kids.... Im lowkey heated..\n",
      "======================================================================\n",
      "@Aryanliving Welcome to the movement. Anyone who gets even minor publicity is a queer, fed, or jew (well, according to the internet).\n",
      "======================================================================\n",
      "RT @BestVineFights: \"When you finally see the white kid that called you nigger on Call Of Duty\" &#128514;&#128557;&#128514;\n",
      "https://t.co/X59D4kMY7P\n",
      "======================================================================\n",
      "@laceeybugg atleast i dont have dick breath you twat licker\n",
      "======================================================================\n",
      "1. Call white person \"white person\" in a derogatory way? That's cool.  Call hispanic person \"white person\" in a derogatory way? Whoa, that's racism!!  Meanwhile white people are being used a placeholder for an insult. Remember when gay people were the token insult? Now, it's being.  I can just hear the kids now, calling eachother white as an insult the way people used to call others faggots as an insult, even if they weren't gay.  So if we're going off the same timeline, it's going to take people about 20-40 years to realize that they have been exhibiting gross racism towards white people.  Hopefully the same Political Correctness rules stay, and people who said racist things about white people in 2018 will lose their jobs in 2040.\n",
      "======================================================================\n",
      "@KCJHoop pretty sure you meant flight and not flite, dumb ass cracker.\n",
      "======================================================================\n",
      "1. If YOU don't homeschool your kids and grandkids it's wilfully giving them to a gang of pedo predators with a box lunch in 2018.   Preschoolers in Iceland are being taught to rid themselves of gender stereotypes by being forced to participate—in gender stereotypes.   Elementary school boys are taught to massage each other with lotion to learn how to have “gentle hands”, and are also taught to play while looking after babies with gender-neutral rag dolls.   As for the boys in the report, Cardew had lit candles and drawn the blinds of their minimalist classroom. At other stations around the room, boys styled each other’s hair, painted on nail polish or gave each other full-body massages, according to the report.   The Hjalli model was founded by self-described radical feminist Margrét Pála Ólafsdóttir who holds no punches when describing why she does what she does.   https://thefreethoughtproject.com/gender-equality-schools-children/    @Wifewithapurpose @lovelymiss @Empress @Mondragon @wocassity @RaviCrux\n",
      "======================================================================\n",
      "1. Nothing infuriates me more than when some retard tries to inform me what my own interests *are* - not *ought to be or should* but let me fucking inform you slave what you fail to recognize for I am your arbiter.  This is the smuggest form of contempt, not even I would ever tell some commie what their interest *is*, at most a persuasive debate or thought experiment.    Some retard has an opinion and that is fine, but to think their own interests represent what I really need is just one step away from thinking I am some animal to be tethered to their control.  Enslaved.  I want an environment where government taxes low, redistributes to the areas that reinforce our freedoms and generally fosters an environment where private enterprises of all sizes can flourish.  I want all the good stuff that comes from freedom and I don't want some massive state overlord styling itself as my God.  When the commies say this is against my own interests and that I really want to take 50% my income and redistribute it to world hunger, \"the environment\" or some state run service is these retards getting high on the methane in their own feces.  \n",
      "======================================================================\n",
      "RT @BenjaminNorton: Remember the \"yellow badge\" Nazis used? \n",
      "Israel is making Muslim women carry a yellow badge order to pray in Al-Aqsa.\n",
      "h&#8230;\n",
      "======================================================================\n",
      "My flow retarded &#128526;\n",
      "======================================================================\n",
      "Lmfao u sloth trinidad james with a relaxer lookin ass bitch\"@Flowy_Bri: i hate niggers, imma start talking to white boys.\"\n",
      "======================================================================\n",
      "Chris Bosh bout to disgrace the trophy like a real faggot smh\n",
      "======================================================================\n",
      "Quan stop RT that faggot on my TL\n",
      "======================================================================\n",
      "I wish faggots like @tyga would stop making music, rack city was fucking awful. Get the fuck outta michigan.\n",
      "======================================================================\n",
      "@aaryneliza im 60 n I hate elissa, a fake ass cunt\n",
      "======================================================================\n",
      "I'll pay yall niggas to get lost, how much y'all hoe niggas gunna cost\n",
      "======================================================================\n",
      "A nigga could tell you he aint fucking with her but really do be , that's why you gotta ask . Most of the time a bitch won't lie about that\n",
      "======================================================================\n",
      "1. @a, we see right through you. This is just you catering to moronic boomercucks like @mitchellvii who won't move to Gab because every retarded thing he says will get BTFO.  The truth fears no investigations.  Just IP ban porn spammer @MartyGraw and the porn problem will be solved. And ban porn in general- it isn't free speech.\n",
      "======================================================================\n",
      "ahah fucking queer @KingJames kill urself\n"
     ]
    }
   ],
   "source": [
    "analysis = rnn_analysis.merge(\n",
    "    softmax_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})\n",
    "\n",
    "# Examples where the softmax model is incorrent, the SVM is right,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "for ex in error_group['raw_examples'].sample(30, random_state=10):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. Grammatical errors, overt racism, child prostitution.. It would appear we have a despicable moron on our hands. \n",
      "======================================================================\n",
      "Chris a faggot\n",
      "======================================================================\n",
      "1. >Progressive activists are the only group that strongly backs political correctness: Only 30 percent see it as a problem.   >   >So  what does this group look like? Compared with the rest of the  (nationally representative) polling sample, progressive activists are  much more likely to be rich, highly educated—and white. They are nearly  twice as likely as the average to make more than $100,000 a year. They  are nearly three times as likely to have a postgraduate degree. And  while 12 percent of the overall sample in the study is African American,  only 3 percent of progressive activists are. **With the exception of the  small tribe of devoted conservatives, progressive activists are the most  racially homogeneous group in the country.**  [AHAHAHAHAHA!](https://www.dailydot.com/wp-content/uploads/146/6e/6d78ec0c8c8eaf31.png)\n",
      "======================================================================\n",
      "1. I just hope the kid ends up a multi millionaire and those fucking little cunts end up behind bars for 10 years minimum and have to register as sex offenders for life, which they won't because of pussy pass.  Feminists want to know why more men are really starting to hate women, this shit is why. These whores will get off with a slap on the wrist at most.\n",
      "======================================================================\n",
      "1. Fucking beach cops in NJ are absolute trash. Patrolling on quads just harassing beach goers minding their own business  The girl did not react properly, but they were both sober and the cop was still trying to bring charges for alcohol that wasn't even opened.  Obstruction is an absolutely bullshit charge.   You can't cuff someone because they don't want to give you their name... She refuses to give her name and he follows her going \"That's it you're getting dropped\"  It's not a crime to not give a police officer your name. It's not a crime to have alcohol in a cooler when none of it is open.  Tackles her, rubs her face in the sand, then starts punching her repeatedly while on top of her?   This dude needs to get fucking fired. \n",
      "======================================================================\n",
      "1. No surprise.All female teachers are feminists.They are indoctrinated in colleges.\n",
      "======================================================================\n",
      "1. Pics or it didn't happen.  Seriously though my ex who pushed me into full mgtow didn't want her breasts touched.  I thought it was very selfish of her which was pretty much her MO anyways.  His reaction before he left tells me he really felt bad about what he had done.  Seems like an honest mistake to me.  Just like a woman, she can't talk to him right away so she calls the police to white knight her hamstered trauma away.\n",
      "======================================================================\n",
      "1. This is the equivalent of being completely alone with another person in a completely silent room and letting out a deafening fart. When they look at you and say \"what the fuck?\", you just say \"what? I didn't let out that hearing-damaging fart.\"  For real, they're either too dishonest or retarded to understand how their own \"where black pplz at?\" comment is a loaded question that implies racism. Nobody is that stupid and it's obvious to everyone else; you just look really stupid by trying to deny it.\n",
      "======================================================================\n",
      "Only the strong survive and that nigga a pussy\n",
      "======================================================================\n",
      "1. Who asked the stupid question?\n"
     ]
    }
   ],
   "source": [
    "# Examples where the softmax model is incorrent, the SVM is right,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'neutral')\n",
    "]\n",
    "for ex in error_group['raw_examples'].sample(10, random_state=10):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-6dc5a2bd1e4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m analysis = nb_analysis.merge(\n\u001b[0m\u001b[0;32m      2\u001b[0m     svm_analysis, left_on='raw_examples', right_on='raw_examples')\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gold_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'gold_x'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'gold'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis = nb_analysis.merge(\n",
    "    softmax_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})\n",
    "\n",
    "# Examples where the nb model is incorrent, the SVM is right,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "\n",
    "for ex in error_group['raw_examples'].sample(10, random_state=10):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-c096e160451b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m analysis = nb_analysis.merge(\n\u001b[0m\u001b[0;32m      2\u001b[0m     rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gold_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'gold_x'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'gold'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis = nb_analysis.merge(\n",
    "    rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})\n",
    "\n",
    "# Examples where the nb model is corect, the RNN is incorrect,\n",
    "# and the gold label is 'hate'\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'hate')\n",
    "]\n",
    "\n",
    "for ex in error_group['raw_examples'].sample(10, random_state=10):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
